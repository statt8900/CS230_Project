{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named ase",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5616c1ca9c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mCNN_input\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCNNInputDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mliteral_eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brohr/Documents/Stanford/Research/scripts/ML/CS230_Final_Project/CNN_input.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#External Modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named ase"
     ]
    }
   ],
   "source": [
    "#External Modules\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pdb\n",
    "from CNN_input import CNNInputDataset\n",
    "from ast import literal_eval\n",
    "\n",
    "def test_forward():\n",
    "    model = nn.Sequential(\n",
    "        ChemConv(2,14),\n",
    "        nn.ReLU(inplace = True),\n",
    "        CollapseAndSum(16)\n",
    "    )\n",
    "\n",
    "    # storage_directories = ['/Users/michaeljstatt/Documents/CS230_Final_Project/data/storage_directories/150868984252']\n",
    "    storage_directories = ['/Users/brohr/Documents/Stanford/Research/scripts/ML/CS230_Final_Project/150868984252']\n",
    "    dataset = CNNInputDataset(storage_directories)\n",
    "    return model(dataset[0])\n",
    "\n",
    "def test_backward():\n",
    "    y_pred = test_forward()\n",
    "    y_actual = Variable(-0.35*torch.ones((1)))\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(y_pred, y_actual)\n",
    "    print y_pred, y_actual, loss\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "def test_that_works():\n",
    "    inp = Variable(torch.randn((10)))\n",
    "    model = nn.Linear(10,1)\n",
    "    y_pred = model(inp)\n",
    "    y_actual = Variable(1.3*torch.ones((1)))\n",
    "    loss_fn = nn.MSELoss()\n",
    "    pdb.set_trace()\n",
    "    loss = loss_fn(y_pred, y_actual)\n",
    "    # print y_pred, y_actual, loss\n",
    "    # pdb.set_trace()\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "class MyReLUFunction(Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return a\n",
    "        Tensor containing the output. You can cache arbitrary Tensors for use in the\n",
    "        backward pass using the save_for_backward method.\n",
    "        \"\"\"\n",
    "        self.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = self.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class ChemConvFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(self, connectivity, node_feature_matrix, filters):\n",
    "        \"\"\"\n",
    "        connectivity is a matrix describing the degree to which each pair of\n",
    "        atoms is connected\n",
    "\n",
    "        node_feature_matrix is size N x F+2 where N is the number of atoms in the cell\n",
    "        and F is the number of filters in the previous conv layer. The 2 indicates that\n",
    "        strength and distance have been included as features at each layer\n",
    "\n",
    "        filters is a matrix of size L x F_prev\n",
    "        where L is the \"number of atoms\" in the filter, and F is the number of filters\n",
    "        in the previous layer.\n",
    "        \"\"\"\n",
    "        N = len(node_feature_matrix)\n",
    "        F = len(filters)\n",
    "        node_connection_matrices = make_convlayer_input_matrix(connectivity,node_feature_matrix)\n",
    "\n",
    "        output = torch.zeros((N, F+2))\n",
    "        for i_node, node_connection_matrix in enumerate(node_connection_matrices):\n",
    "            for i_filter, f in enumerate(filters):\n",
    "                output[i_node, i_filter] = convolution_operation(node_connection_matrix, f)\n",
    "\n",
    "        self.save_for_backward(connectivity, node_feature_matrix, filters, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "class ChemConv(nn.Module):\n",
    "    def __init__(self, in_depth, out_depth):\n",
    "        super(ChemConv, self).__init__()\n",
    "        self.in_depth = in_depth\n",
    "        self.out_depth = out_depth\n",
    "\n",
    "        ######FILTER DIMENSION#########\n",
    "        filter_dimension = 12\n",
    "        self.filters = torch.randn(out_depth,filter_dimension,in_depth+2)*0.01\n",
    "\n",
    "    def forward(self, input):\n",
    "        (connectivity, node_feature_matrix, energy) = input\n",
    "        return ChemConvFunction.apply(connectivity, node_feature_matrix, self.filters)\n",
    "\n",
    "class CollapseAndSum(nn.Module):\n",
    "    def __init__(self, in_depth):\n",
    "        super(CollapseAndSum, self).__init__()\n",
    "        self.in_depth = in_depth\n",
    "        self.linear = nn.Linear(in_depth, 1, bias=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        N = input.shape[0]\n",
    "        output = torch.zeros((N))\n",
    "        for i in range(N):\n",
    "            output[i] = self.linear(input[i]).data[0]\n",
    "\n",
    "        output = torch.sum(output)\n",
    "        output = Variable(output*torch.ones((1)))\n",
    "        return output\n",
    "\n",
    "\n",
    "def convolution_operation(node_connection_matrix,filt):\n",
    "    ordered_connection_matrix = order_input(node_connection_matrix, filt)\n",
    "    if ordered_connection_matrix.shape[0] < filt.shape[0]:\n",
    "        filt = filt[:ordered_connection_matrix.shape[0]]\n",
    "    return torch.sum(torch.mul(ordered_connection_matrix,filt))\n",
    "\n",
    "def order_input(node_connection_matrix,filt):\n",
    "    \"\"\"\n",
    "    node_connection_matrix ::  e x F+1 matrix, where e is the number of edges of the\n",
    "                        node that we are applying filter to and F is the number\n",
    "                        of filters in the previous convlayer (or 2 for init data)\n",
    "\n",
    "    filter        :: fx x fy matrix, where fx is the arity of the filter\n",
    "                       and fy is the number of edges captured by the filter\n",
    "\n",
    "    NOTE: for speed, we could build up the convolution operation inside the\n",
    "        for loop (conv += np.dot(node_connection_matrix[best_fit]\n",
    "\n",
    "    \"\"\"\n",
    "    node_connection_tensor = torch.from_numpy(node_connection_matrix)\n",
    "    output_dimensions = (min(node_connection_matrix.shape[0]-1,filt.shape[0]),filt.shape[1])\n",
    "    output = torch.zeros(output_dimensions)\n",
    "    output[0] = node_connection_tensor[0]\n",
    "\n",
    "    if len(filt)>0:\n",
    "        i = 1\n",
    "        for filtrow in filt[1:]: # presuming no atoms have NO bonds\n",
    "            if i<output.shape[0]:\n",
    "                scores                  = torch.matmul(node_connection_tensor,filtrow.double())\n",
    "                best_fit                = np.argmax(scores)\n",
    "                output[i]               = node_connection_tensor[best_fit]\n",
    "                filtered_numpy          = np.delete(np.array(node_connection_tensor),best_fit,0)\n",
    "                node_connection_tensor  = torch.from_numpy(filtered_numpy)\n",
    "            i+=1\n",
    "    return output\n",
    "\n",
    "def make_convlayer_input_matrix(connectivity,node_feature_matrix):\n",
    "    \"\"\"\n",
    "    Takes a connectivity list and node_feature matrix to produce an input list\n",
    "    (of np arrays) for the conv layer\n",
    "\n",
    "    connectivity :: [?x3] (list of length N)\n",
    "    node_feature :: NxF matrix of features\n",
    "    output ::[?xF+2] (list of length N)\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for i,connections in enumerate(connectivity):\n",
    "        this_node = np.append(node_feature_matrix[i],[0,0])\n",
    "        newatom = [this_node]\n",
    "        for to_node, strength, dist in connections:\n",
    "            node_feature = node_feature_matrix[int(to_node)]\n",
    "            newatom.append(np.append(node_feature,[strength,dist])) # num_features + 2\n",
    "        output.append(np.array(newatom))\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
