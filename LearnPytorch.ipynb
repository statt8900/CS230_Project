{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#External Modules\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pdb\n",
    "from CNN_input import CNNInputDataset\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_forward():\n",
    "    model = nn.Sequential(\n",
    "        ChemConv(2,14),\n",
    "        nn.ReLU(inplace = True),\n",
    "        nn.Linear(16,1, bias=True),\n",
    "        Average()\n",
    "    )\n",
    "\n",
    "    # storage_directories = ['/Users/michaeljstatt/Documents/CS230_Final_Project/data/storage_directories/150868984252']\n",
    "    storage_directories = ['/Users/brohr/Documents/Stanford/Research/scripts/ML/CS230_Final_Project/150868984252']\n",
    "    dataset = CNNInputDataset(storage_directories)\n",
    "    return model(dataset[0])\n",
    "\n",
    "def test_backward():\n",
    "    y_pred = test_forward()\n",
    "    y_actual = Variable(-0.35*torch.ones((1)))\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(y_pred, y_actual)\n",
    "    print y_pred, y_actual, loss\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "def test_that_works():\n",
    "    inp = Variable(torch.randn((10)))\n",
    "    model = nn.Linear(10,1)\n",
    "    y_pred = model(inp)\n",
    "    y_actual = Variable(1.3*torch.ones((1)))\n",
    "    loss_fn = nn.MSELoss()\n",
    "    pdb.set_trace()\n",
    "    loss = loss_fn(y_pred, y_actual)\n",
    "    # print y_pred, y_actual, loss\n",
    "    # pdb.set_trace()\n",
    "    loss.backward()\n",
    "    \n",
    "def store_params(model):\n",
    "    p = []\n",
    "    for i in model.parameters():\n",
    "        p.append(i)\n",
    "        \n",
    "    return p\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ChemConvFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, connectivity, node_feature_matrix, filters):\n",
    "        \"\"\"\n",
    "        connectivity is a matrix describing the degree to which each pair of\n",
    "        atoms is connected\n",
    "\n",
    "        node_feature_matrix is size N x F+2 where N is the number of atoms in the cell\n",
    "        and F is the number of filters in the previous conv layer. The 2 indicates that\n",
    "        strength and distance have been included as features at each layer\n",
    "\n",
    "        filters is a matrix of size L x F_prev\n",
    "        where L is the \"number of atoms\" in the filter, and F is the number of filters\n",
    "        in the previous layer.\n",
    "        \"\"\"\n",
    "        N = len(node_feature_matrix)\n",
    "        F = len(filters)\n",
    "        node_connection_matrices, atom_index_vectors = make_convlayer_input_matrix(connectivity,node_feature_matrix)\n",
    "        output = torch.zeros((N, F+2))\n",
    "        ctx.ordered_connection_matrices = []\n",
    "        ctx.atom_index_vectors = []\n",
    "        ctx.filters_used = []\n",
    "\n",
    "        for i_node, (node_connection_matrix, atom_index_vector) in enumerate(zip(node_connection_matrices, atom_index_vectors)):\n",
    "            for i_filter, f in enumerate(filters):\n",
    "                conv_result, ordered_connection_matrix, atom_index_vector, filter_used = convolution_operation(node_connection_matrix, atom_index_vector, f)\n",
    "                ctx.ordered_connection_matrices.append(ordered_connection_matrix)\n",
    "                ctx.atom_index_vectors.append(atom_index_vector)\n",
    "                ctx.filters_used.append(filter_used)\n",
    "                output[i_node, i_filter] = conv_result\n",
    "\n",
    "#         ctx.save_for_backward(connectivity, node_feature_matrix, filters, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        raise NotImplementedError\n",
    "        grad_input = None\n",
    "        grad_filters = None\n",
    "        for a,b,c in zip(ctx.ordered_connection_matrices, ctx.atom_index_vectors, ctx.filters_used):\n",
    "            pass # some += statement or statements\n",
    "            \n",
    "        return grad_input, grad_filters\n",
    "        \n",
    "\n",
    "        \n",
    "def convolution_operation(node_connection_matrix, atom_index_vector, filt):\n",
    "    ordered_connection_matrix, atom_index_vector = order_input(node_connection_matrix, atom_index_vector, filt)\n",
    "    if ordered_connection_matrix.shape[0] < filt.shape[0]:\n",
    "        filt = filt[:ordered_connection_matrix.shape[0]]\n",
    "    output = torch.sum(torch.mul(ordered_connection_matrix,filt))\n",
    "    return output, ordered_connection_matrix, atom_index_vector, filt\n",
    "\n",
    "\n",
    "\n",
    "class ChemConv(nn.Module):\n",
    "    def __init__(self, in_depth, out_depth):\n",
    "        super(ChemConv, self).__init__()\n",
    "        self.in_depth = in_depth\n",
    "        self.out_depth = out_depth\n",
    "\n",
    "        ######FILTER DIMENSION#########\n",
    "        filter_dimension = 13\n",
    "        self.filters = nn.Parameter(torch.Tensor(out_depth,filter_dimension,in_depth+2))\n",
    "        self.filters.data.normal_()\n",
    "        self.filters.data *= 0.01\n",
    "\n",
    "    def forward(self, input):\n",
    "        (connectivity, node_feature_matrix, energy) = input\n",
    "        return ChemConvFunction.apply(connectivity, node_feature_matrix, self.filters)\n",
    "\n",
    "\n",
    "def order_input(node_connection_matrix, atom_index_vector, filt):\n",
    "    \"\"\"\n",
    "    node_connection_matrix ::  e x F+1 matrix, where e is the number of edges of the\n",
    "                        node that we are applying filter to and F is the number\n",
    "                        of filters in the previous convlayer (or 2 for init data)\n",
    "\n",
    "    filter        :: fx x fy matrix, where fx is the arity of the filter\n",
    "                       and fy is the number of edges captured by the filter\n",
    "\n",
    "    NOTE: for speed, we could build up the convolution operation inside the\n",
    "        for loop (conv += np.dot(node_connection_matrix[best_fit]\n",
    "\n",
    "    \"\"\"\n",
    "    sum_check = sum(atom_index_vector) # for assert check at end of function\n",
    "    node_connection_tensor = torch.from_numpy(node_connection_matrix)\n",
    "    output_dimensions = (min(node_connection_matrix.shape[0],filt.shape[0]),filt.shape[1]) #smaller of (num edges this node has, length of filter)\n",
    "    output = torch.zeros(output_dimensions)\n",
    "    output[0] = node_connection_tensor[0]\n",
    "    filtered_numpy = np.delete(np.array(node_connection_tensor),0,0) # Brian\n",
    "    \n",
    "    ordered_atom_index_vector = [] # Brian\n",
    "    if len(filt)>0:\n",
    "        i = 1\n",
    "        ordered_atom_index_vector.append(atom_index_vector[0]) # Brian\n",
    "        del atom_index_vector[0] # Brian\n",
    "        for filtrow in filt[1:]: # presuming no atoms have NO bonds\n",
    "            if i<output.shape[0]:\n",
    "                node_connection_tensor  = torch.from_numpy(filtered_numpy) # Brian\n",
    "                scores                  = torch.matmul(node_connection_tensor,filtrow.double())\n",
    "                best_fit                = np.argmax(scores)\n",
    "                ordered_atom_index_vector.append(atom_index_vector[best_fit]) # Brian\n",
    "                del atom_index_vector[best_fit] # Brian\n",
    "                output[i]               = node_connection_tensor[best_fit]\n",
    "                filtered_numpy          = np.delete(np.array(node_connection_tensor),best_fit,0)\n",
    "            i+=1\n",
    "            \n",
    "    assert sum_check == sum(ordered_atom_index_vector) # these vectors should contain the same indices in a possibly different order, so they should have the same sum\n",
    "    return output, ordered_atom_index_vector\n",
    "\n",
    "def make_convlayer_input_matrix(connectivity,node_feature_matrix):\n",
    "    \"\"\"\n",
    "    Takes a connectivity list and node_feature matrix to produce an input list\n",
    "    (of np arrays) for the conv layer\n",
    "\n",
    "    connectivity :: [?x4] (list of length N)\n",
    "    node_feature :: NxF matrix of features\n",
    "    output ::[?xF+2] (list of length N)\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    atom_index_vectors = []\n",
    "    for i,connections in enumerate(connectivity):\n",
    "        this_node = np.append(node_feature_matrix[i],[0,0])\n",
    "        newatom = [this_node]\n",
    "        atom_index_vector = [i]\n",
    "        for to_node, strength, dist in connections:\n",
    "            node_feature = node_feature_matrix[int(to_node)]\n",
    "            atom_index_vector.append(int(to_node))\n",
    "            newatom.append(np.append(node_feature,[strength,dist])) # num_features + 2\n",
    "        output.append(np.array(newatom))\n",
    "        atom_index_vectors.append(atom_index_vector)\n",
    "        assert len(atom_index_vector) == len(newatom)\n",
    "    assert len(atom_index_vectors) == len(output)\n",
    "    return output, atom_index_vectors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.3725\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-477-9adbea2d6756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brohr/Documents/Stanford/Research/scripts/virtalenvs/ase314/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brohr/Documents/Stanford/Research/scripts/virtalenvs/ase314/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brohr/Documents/Stanford/Research/scripts/virtalenvs/ase314/lib/python2.7/site-packages/torch/autograd/function.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-476-19f062c5785a>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mgrad_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mgrad_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### ChemConv Test\n",
    "model = nn.Sequential(\n",
    "    ChemConv(2,14),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Linear(16,1, bias=True),\n",
    "    Average()\n",
    ")\n",
    "\n",
    "# storage_directories = ['/Users/michaeljstatt/Documents/CS230_Final_Project/data/storage_directories/150868984252']\n",
    "storage_directories = ['/Users/brohr/Documents/Stanford/Research/scripts/ML/CS230_Final_Project/150868984252']\n",
    "dataset = CNNInputDataset(storage_directories)\n",
    "y_pred = model(dataset[0])\n",
    "print y_pred\n",
    "# pdb.set_trace()\n",
    "y_actual = Variable(-0.35*torch.ones((1)))\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(y_pred, y_actual)\n",
    "# print y_pred, y_actual, loss\n",
    "p=1\n",
    "p = store_params(model)\n",
    "loss.backward()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearFunction(Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.matmul(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_variables\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.matmul(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "    \n",
    "class MyLinearModule(nn.Module):\n",
    "    def __init__(self, input_features, output_features, bias=True):\n",
    "        super(MyLinearModule, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        # nn.Parameter is a special kind of Variable, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters can never be volatile and, different than Variables,\n",
    "        # they require gradients by default.\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_features, input_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        self.weight.data.uniform_(-0.1, 0.1)\n",
    "        if bias is not None:\n",
    "            self.bias.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # See the autograd section for explanation of what happens here.\n",
    "        return MyLinearFunction.apply(input, self.weight, self.bias)\n",
    "    \n",
    "\n",
    "    \n",
    "inp = Variable(torch.randn((1,10)))\n",
    "model = MyLinearModule(10,1)\n",
    "y_pred = model(inp)\n",
    "y_actual = Variable(1.3*torch.ones((1)))\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(y_pred, y_actual)\n",
    "# print y_pred, y_actual, loss\n",
    "# pdb.set_trace()\n",
    "loss.backward()\n",
    "\n",
    "# f = y_pred.grad_fn\n",
    "# print f\n",
    "# f.apply(2)\n",
    "\n",
    "p = store_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Average Test\n",
    "class AverageFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.N = input.shape[0]\n",
    "        output = torch.mean(input)\n",
    "        output = output*torch.ones(1)\n",
    "        return output\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = Variable(grad_output.data/float(ctx.N)*torch.ones(ctx.N,1))\n",
    "        return grad_input\n",
    "        \n",
    "    \n",
    "class Average(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Average, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return AverageFunction.apply(input)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Variable(torch.randn((10,5))) #10 atoms (N), 5 descriptors (D)\n",
    "\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(5,1, bias=True),\n",
    "#     Average()\n",
    "# )\n",
    "\n",
    "model = nn.Sequential(\n",
    "    MyLinearModule(5,1, bias=True),\n",
    "    Average()\n",
    ")\n",
    "\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     MyLinearModule(5,1, bias=True)\n",
    "# )\n",
    "\n",
    "\n",
    "y_pred = model(inp)\n",
    "# y_pred.backward()\n",
    "y_actual = Variable(1.3*torch.ones((1)))\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(y_pred, y_actual)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ase314",
   "language": "python",
   "name": "ase314"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
